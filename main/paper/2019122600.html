<!doctype html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<title>[appdix]-2 Stochastic Optimization for Large-scale Optimal Transport</title>
</head>
<body>
<article>我们首先证明 $ \mathcal{P_\varepsilon} $ 和 $ \mathcal{D_\varepsilon} $ 的等价性.

下面的证明参考的是 A user's guide to optimal transport P12 的证明，作者所使用的证明技巧虽然没有定理1.13那么严谨，但是非常巧妙，可以无缝地应用到这里得到结论. 这本书称： The calculations we are going to do are very common in linear programming, 但是没有给出参考文献，真的想看一下出处在哪啊，实在太巧妙了！神来之笔.

所以先别急着去证明命题2.1，我先解读一下 A user's guide to optimal transport P13 中的证明.

## OT duality 的证明

这部分内容比较多，我把内容抽出来了，参见我的博文：再谈OT对偶性的证明
里面主要用了 lagrange duality 和 fenchel duality 两种方法来证明 OT 对偶性.

## 回到 EOT

### 拉格朗日乘子法

这一节，我们证明一下 $ \mathcal{P_\varepsilon} = \mathcal{D_\varepsilon} $.

对于EOT，其实主要问题还是如何计算得到其 lagrange dual function. 我们还是使用在 Lagrange duality 中的老一套. $ \inf\limits_{\pi\in \mathcal{\Pi(X\times Y)}} \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) \\= \inf\limits_{\pi\in \mathcal{M(X\times Y)}} \sup\limits_{u\in \mathcal{C_b(X)}, v\in \mathcal{C_b(Y)}} \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) + [\int u(x)d\mu(x) + \int v(y) d\nu(y) - \int (u+v)d\pi(x, y)] \\= \sup\limits_{u\in \mathcal{C_b(X)}, v\in \mathcal{C_b(Y)}} \inf\limits_{\pi\in \mathcal{M(X\times Y)}} \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) + [\int u(x)d\mu(x) + \int v(y) d\nu(y) - \int (u+v)d\pi(x, y)] \\= \sup\limits_{u\in \mathcal{C_b(X)}, v\in \mathcal{C_b(Y)}} \int u(x)d\mu(x) + \int v(y) d\nu(y) + \inf\limits_{\pi\in \mathcal{M(X\times Y)}} [ \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) - \int (u+v)d\pi(x, y)] $

第一个等号是拉格朗日乘子法的结果. 第二个等号是 strong duality (by min-max principle) 的结果.

下面，我们重点考察 $\inf\limits_{\pi\in \mathcal{M(X\times Y)}} [\int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) - \int (u+v)d\pi(x, y)] $

前面我们已经纠正过了，$\text{KL}(\pi|\mu\otimes \nu) = \int (\log (\frac{d\pi}{d\mu d\nu}) - 1) d\pi$. 为了方便，我们记 $R_\pi(x, y) = \frac{d\pi(x, y)}{d\mu(x) d\nu(y)}$

那么，我们有

$ \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) - \int (u+v)d\pi(x, y) = \int_{\mathcal{X\times Y}} [c(x, y)R_\pi + \varepsilon R_\pi(\log R_\pi - 1) - (u+v)R_\pi]d\mu d\nu $

那么

$\inf\limits_{\pi\in \mathcal{M(X\times Y)}} [ \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) - \int (u+v)d\pi(x, y)] = \inf\limits_{R_\pi\in \mathcal{C_b(X\times Y)}} \int_{\mathcal{X\times Y}} [c(x, y)R_\pi + \varepsilon R_\pi(\log R_\pi - 1) - (u+v)R_\pi]d\mu d\nu $

对 $R_\pi$ 求导，且令导数为0，有
$ c(x, y) - u(x) - v(y) \varepsilon \log R_\pi = 0 $

故 $R_\pi = \exp{\frac{u+v-c}{\varepsilon}}$ 时，$\inf\limits_{\pi\in \mathcal{M(X\times Y)}} [ c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) - \int (u+v)d\pi(x, y)]$ 取最小值，且最小值为 $ -\varepsilon R_\pi d\mu d\nu $. 如果不纠正 KL 的定义，这一步会出现问题.

因此，我们得到：

$ \inf\limits_{\pi\in \mathcal{\Pi(X\times Y)}} \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) = \sup\limits_{u\in \mathcal{C_b(X)}, v\in \mathcal{C_b(Y)}} \int u(x)d\mu(x) + \int v(y) d\nu(y) - \varepsilon\int_{\mathcal{X\times Y}} R_\pi d\mu d\nu $

很巧的是，$- \varepsilon\int_{\mathcal{X\times Y}} R_\pi d\mu d\nu$ 恰好就是 $\iota_{U_c}$ 的光滑化 $ \iota_{U_c}^\varepsilon $ . 到底是什么造成了这样的巧合？内中有深意！

这样一来，我们就成功证到了 $ \mathcal{P_\varepsilon} = \mathcal{D_\varepsilon} $.

#### KKT 条件

我们检查一下此时的 KKT 条件. 来看看 $u^*, v^*$ 和 $\pi^*$ 之间的关系. 主要是检查 Stationarity. 可以看到此时的 Lagrangian 是 $L(\pi, u, v) = \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) + \int u(x)d\mu(x) + \int v(y) d\nu(y) - \int (u+v)d\pi(x, y) = \int u(x)d\mu(x) + \int v(y) d\nu(y) + \int_{\mathcal{X\times Y}} [c(x, y)R_\pi + \varepsilon R_\pi(\log R_\pi - 1) - (u+v)R_\pi]d\mu d\nu $

则 $ \partial_{R_\pi} L(\pi, u, v) = \int_{\mathcal{X\times Y}} [c(x, y) -u(x) - v(y) + \varepsilon \log R_\pi ]d\mu d\nu $

令之为 0,
则有 $R_{\pi^*} = \exp(\frac{u^*+v^*-c}{\varepsilon})$

从而有 $d\pi^* = R_{\pi^*} d\mu d\nu = \exp{\frac{u^*+v^*-c}{\varepsilon}} d\mu d\nu$

从而，我们看到，我们可以通过 $u^*$ 和 $v^*$ 来显式恢复出 $\pi^*$. 而我们先前在 original OT 的 KKT 条件中就得不到这么好的结果.

因此，某种意义上，EOT 是比 OT 更加强大的工具.

除此之外，我们还可以获得一个副产品，那就是 $ \exp{\frac{u^*+v^*-c}{\varepsilon}} d\mu d\nu \in \Pi(\mu, \nu) $

这个结果可不是那么平凡的哦(○｀ 3′○) 如果不是通过 KKT 条件，直接让你证
$\exp{\frac{u^*+v^*-c}{\varepsilon}} d\mu d\nu \in \Pi(\mu, \nu) $
还真不是那么容易的事情.

我们仔细观察一下上面的 KKT 条件，我们可以发现，$ \partial_{R_\pi} L(\pi, u, v) $ 其实就是这部分式子的导数，看图中圈出来的部分：

![](https://raw.githubusercontent.com/ai-math/picbed/master/1546837286759_2.png)

看清楚了吗！！！没想到最优的 $\pi$ 恰巧求解了那部分 inf 问题！！不，更让我吃惊的是，给定 $u=u^*, v=v^*$, 求解这个 inf 问题的最优的 $\pi^*$, 竟然恰好满足约束 $ \pi^*\in\Pi(\mu,\nu) $, 打死我也不敢相信这一点啊.

而且，我们可以看到，对于original OT, 这部分的导数是 $c-u^*-v^*$, 所以，对于 original OT 来说，是没办法直接通过 $ u^*, v^* $ 恢复出来的.

### 原文证法解读：fenchel duality

![](https://raw.githubusercontent.com/ai-math/picbed/master/1546837287096_3.png)

https://en.wikipedia.org/wiki/Fenchel%27s_duality_theorem

paper 中显然使用的是 fenchel duality.
关键在于如何构造 $f$ 和 $g$. 下面我们按照步骤来看看如何利用 fenchel duality.
1) 确定 fenchel primal space. 显然这里还是 $C(X\times Y)$.

这样一来，在当前的场景中，只能从 fenchel dual form 出发，即从 $ \inf\limits_{\pi\in \mathcal{\Pi(X\times Y)}} \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) $ 出发.

2) 约束 $\to$ 指示函数. 代入到 fenchel duality 的式子当中去. 可得

优化目标函数： $ g^*(-\pi) = \begin{cases} \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) & \pi \in M(X\times Y) \\ +\infty & otherwise \end{cases} $

指示函数： $ f^*(\pi) = \begin{cases} 0 & \pi \in \Pi(\mu, \nu) \\ +\infty & otherwise \end{cases} $

3) 构造 $f$ 和 $g$.
首先，对于 $f^*$ 来说，我们瞻前顾后一下，就知道它对应的 $f$ 是：

$ f(u) = \begin{cases} \int_X \varphi(x) d\mu + \int_Y \psi(y) d\nu & u = \varphi + \psi \\ +\infty & otherwise \end{cases} $

所以我们只需要找到 $g$ 即可. 那么，可以使用 biconjugate 的性质.

$ g(u) = g^{**}(u) = (g^*(\pi))^* = \sup\limits_{-\pi\in M(X\times Y)} [\langle u, -\pi angle - g^*(-\pi)] \\= \sup\limits_{\pi\in M(X\times Y)} [-\int ud\pi - \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) - \varepsilon \text{KL}(\pi|\mu\otimes \nu)] \\= \varepsilon \int_{\mathcal{X\times Y}}\exp(\frac{-u(x,y)-c(x,y)}{\varepsilon})d\mu(x)d\nu(y) $

到这里为止，利用 fenchel duality 来证明命题 2.1 就算完成了.

证明 fenchel duality 的主要思想还是优化目标转约束，约束转优化目标。只不过由于在 dual fenchel problem 的优化目标中引入了额外的 KL 散度，导致 primal fenchel problem 的 hard constraint 变成了 soft constraint. 如果 $\varepsilon = 0$, 那么

$ \sup\limits_{\pi\in M(X\times Y)} [-\int ud\pi - \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y)] = \begin{cases} 0 & -u \le c \\ +\infty & otherwise \end{cases} $

这时候，我们看到 primal fenchel problem 的(hard)约束还是存在的. 所以为什么我们说 EOT 是光滑的呢？我们为什么说 KL 散度的引入导致 dual EOT 是光滑的呢？就是因为通过引入额外的项，使得 primal EOT 的优化目标的 convex conjugate 是光滑的，而不再是先前那样的指示函数，在边界处发生了巨大的跃迁.

其实，通过 sup 函数是非常容易产生指示函数，例如，对于这种类型的函数 $h(u, c)f(\pi)$, 如果 $f(\pi)$ 的最大值为 $+\infty$, 最小值是 $0$, 那么 $\sup_\pi h(u, c)f(\pi)$ 自然就变成了约束 $ h(u, c) \le 0 $ 的指示函数. 注意，最小值只能为 0, 否则就会导致 $\sup_\pi h(u, c)f(\pi)$ 取 $0$ 和 $+\infty$ 以外的值.

我们在这里讨论一下加入的 KL 散度起到的作用. 请注意，无论是在 lagrange duality 的证明中，还是 fenchel duality 的证明中，KL散度起作用的部分都是这个式子（最多相差个正负号）

$ \sup\limits_{\pi\in M(X\times Y)} [-\int ud\pi - \int_{\mathcal{X\times Y}} c(x, y)d\pi(x,y) - \varepsilon \text{KL}(\pi|\mu\otimes \nu)] $

KL 散度起到将 sup 函数光滑化的作用.
----------
补充一些讨论.

1, 我们前面说过，考虑将约束 $ d\pi = R_\pi(x, y)d\mu d\nu $ 加到 $g^*(-\pi)$ 里面，这样看起来好像 $f(u)$ 和 $g(u)$ 都没变？

带入计算，可以发现 $g^{**}(u)$ 确实没改变，但是此时 $g^{***}(-\pi) \ne g^*(-\pi)$. 也就是这时候问题不符合规范，因为处理的空间不再是严格对偶的. 需要对 $g^{**}(u)$ 中的对偶空间进行修正，才能使得 $g^{***}(-\pi) = g^*(-\pi)$

#### fenchel duality 的 KKT 条件

对应于 lagrange duality 的 KKT 条件，我们发现 $\pi^*$ 是函数 $[ \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) - \int (\varphi^*+\psi^*)d\pi(x, y)]$ 的极值点，

也就是说 $\pi^*$ 是求解

$g(u^*) = \sup\limits_{-\pi\in M(X\times Y)} [\langle u^*, -\pi angle - g^*(-\pi)]$ 问题时的极值点.

特别地，由 danskin 定理，我们知道 $\pi^* = -\nabla g(u^*)$, 这里 $(-u^*) = (\varphi^*+\psi^*)$ . 但是我们一般更倾向于考虑正则项的凸共轭，因为直接 计算正则项的凸共轭就不再需要拖着 $\int c(x,y)d\pi$ 这个尾巴了：

如果我们记 $\Omega_\varepsilon(\pi) = \varepsilon \text{KL}(\pi|\mu\otimes \nu)$, 那么有 $g(u^*) = \sup\limits_{-\pi\in M(X\times Y)} [\langle u^*, -\pi angle - g^*(-\pi)] \\= \sup\limits_{\pi\in M(X\times Y)} [\langle -u^*-c, \pi angle - \Omega_\varepsilon(\pi)] \\= \Omega_\varepsilon^*(-u^*-c) $

所以有 $ \pi^* = \nabla \Omega_\varepsilon^*(-u^*-c) $. 如果 $\nabla \Omega_\varepsilon^*(\cdot)$ 具有闭形式，那么代入 $-u^*-c$, 我们可以直接恢复出 $\pi^*$. 其中这里的 $ (-u^*) = (\varphi^*+\psi^*) $.

**请注意符号，千万不要搞错**。如果没有把握，那就使用式子：

$\inf\limits_{\pi\in \mathcal{M(X\times Y)}} [ \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon \text{KL}(\pi|\mu\otimes \nu) - \int (u^*+v^*)d\pi(x, y)]$.

反正结果是一致的.
**总结一下**，fenchel duality 的 KKT 条件是，$\pi^*$ 是求解

$g(u^*) = \sup\limits_{-\pi\in M(X\times Y)} [\langle u^*, -\pi angle - g^*(-\pi)]$

问题时的极值点.

More tricky, $ \pi^* = \nabla \Omega_\varepsilon^*(\varphi^*+\psi^*-c) $, 其中 $ \Omega_\varepsilon^* $ 是正则项 $\Omega_\varepsilon$ 的凸共轭.

**适用情况**讨论：
令 $S(\pi, \Omega) = \int cd\pi + \Omega(\pi) - \int (\varphi^*+\psi^*)d\pi$

前 3 条是 genneral 的，后 3 条是关于 danskin 定理在 KKT 条件中的使用. KKT 条件是基于极值点的，danskin 定理是基于最值点的，所以 danskin 定理不总是适用的.

1) 我们上面的 KKT 条件都是基于**极值点**的，换句话说，如果 $S(\pi, \Omega)$ 的极值点不存在，那么上面的讨论就没有什么意义了. 我再强调一遍，上面的讨论是极值点的，有些时候， $S(\pi, \Omega)$ 的极值点不存在，但是存在最值点，例如 $\Omega(\pi) = 0$ 时，$S(\pi, \Omega)$ 没有极值点，但是有最值点 $ \pi = 0 $, 但是最值点对于 KKT 条件是没有意义的.

2) 一般情况下，我们认为 $S(\pi, \Omega)$ 中的 $\pi$ 是无约束的，可以取任何值的. 但是有些时候 $R(\pi)$ 会对 $\pi$ 产生约束，例如 $\log \pi$ 要求 $\pi \ge 0$. 那么如果 $S(\pi, \Omega)$ 的极值点存在负的分量，那么自然上面的KKT条件也就不成立.

3) 运用驻点法找极值点的时候，你找到的有可能是鞍点；即便是极值点，也未必满足问题条件；所以需要对解的有效性进行验证. 而且 KKT 条件里面除了 Stationarity 条件外，还有 Primal feasibility, Dual feasibility 和 Complementary slackness 需要进行验证.

4) danskin定理是在唯一最值点处才有 $\nabla_x f(x;y) = \nabla_x \phi(x, y)$. 所以，只有当极值点 $\pi$ 同时还是最值点的时候，danskin 定理才能运用到 KKT 条件当中. 否则，由 danskin 定理得出的结论和 KKT 条件没有半毛钱关系.

5) 如果 $S(\pi, \Omega)$ 存在多个最值点，那么 danskin 定理的形式就要改一改.
6) danskin 定理还有可导性要求，如果相关的函数不存在（方向）导数，那么 danskin 定理还是无法应用.

对于第 4 点，我举个例子，让你印象深刻一点. 例如还是当 $\Omega(\pi) = 0$, $S(\pi, \Omega)$ 没有极值点，但是有最值点 $ \pi = 0 $. 因为有最值点，所以可以使用 danskin 定理，也就是有 $ 0 = \nabla 0^*(-u^*-c) $. ($\nabla 0^*(\cdot)$ 除了 0 这点，其他地方都为 0). 但是 $ \pi = 0 $ 是最值点，而不是极值点，所以和 KKT 条件没有半毛钱关系，$\pi^*$ 也不会等于 0.

所以用的时候要小心一点，只要知道了本质，自然就知道如何去处理。不同的处理方式在不同条件下的计算代价是不同的，所以运用之妙，存乎一心.

例如，有的时候 $\nabla \Omega^*(\cdot)$ 是有闭形式的，那么我们在选择 $\Omega$ 的时候，尽量希望 $S(\pi, \Omega)$ 具有唯一的极值点，并且这个极值点还是唯一的最值点，这样一来，我们就能在 KKT 条件中使用 danskin 定理，那么求取 $\pi^*$ 也就方便多了.

一般来说，只要 $\Omega$ 是严格凸的，就能保证$S(\pi, \Omega)$ 具有唯一的极值点，并且这个极值点还是唯一的最值点. 但是**我们一般都假设 $\Omega$ 是强凸的**，因为强凸性还能保证 $\Omega^*$ 的光滑性，因此 $\Omega^*$ 的导数一定存在，从而完全满足 danskin 定理在 KKT 定理中的最佳使用条件！

#### 正则项的选取

##### 指导原则

在 fenchel duality 的 KKT 条件 这一节，我们已经看到一些本质。因此，我们在选择正则项 $\Omega$ 的时候，尽量希望 $S(\pi, \Omega)$ 具有唯一的极值点，并且这个极值点还是唯一的最值点，这样一来有利于获得 $\pi^*$ 和 $\varphi^*$ 和 $\psi^*$ 之间的显式关系.

其实只要 $\Omega(\pi)$ 是全空间取值的，strictly convex 的函数就可以满足上面这些条件.

当然，选择正则项 $\Omega$ 的时候，我们还希望它的凸共轭 $\Omega^*$ 是光滑、可导的. 这一点只要 $\Omega$ 是强凸的，就能保证 $\Omega^*$ 是光滑的，因此也就不可能是指示函数.

所以，一个全空间取值的、strictly convex、strongly convex 的正则项即为所求. 事实上，**一般要求正则项 strongly convex 就够用了**. 因为强凸函数的极值点肯定是唯一的，也肯定是唯一的最值点. 一般来说，全空间取值问题也不大，从而保证极值点是存在的. 再加上 $\Omega^*$ 是凸光滑可微的，还要啥自行车？danskin 定理这种还不是随便用.

##### 处理技巧 I

该技巧在OT相关的论文中是数见不鲜了.
在这里，我们可以使用一种新的观点来看待项 $g(u)$:

$ g(u) = \sup\limits_{\pi\in M(X\times Y)} [-\int ud\pi - \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) - \Omega(\pi))] \\= \sup\limits_{\pi\in M(X\times Y)}[ -\int (u+c)d\pi - \Omega(\pi)) ] \\= \Omega^*(-(u+c)) $

where $\Omega^*$ 是 $\Omega$ 的凸共轭. 那么换句话说，我们其实是需要找一个这样的正则化项 $\Omega(\pi)$ ，它的凸共轭是光滑的. 这样一想，是不是有点豁然开朗的味道？

如果不加正则项，那就是 $\Omega_\pi = 0$, 而 0 的凸共轭恰恰是恶心人的指示函数 $ 0^*(-(u+c)) $ . 那么 $ \Omega^*(-(u+c)) $ 不是恰好可以看作是 indicator function $0^*(-(u+c))$ 的一个松弛吗！！！o(\*^▽^\*)┛

那么什么样的函数，他的凸共轭是光滑的呢？可以从 $\pi$ 离散的case获得灵感.
有一个结论，strongly convex 函数的凸共轭一定是光滑的.
总结一下：
**所谓的优化目标转约束，最后可以转换为求 regularization term 的 convex conjugate 的问题**.

##### 处理技巧 II

另外一个技巧就是缩小我们需要考虑的 $\Omega(\pi)$ 的范围，我们就认为 $\Omega(\pi) = \varepsilon \int T(\pi)d\pi$. 那么问题就转换为如何引入额外的正则项 $T(\pi)$，s.t. $\sup\limits_{\pi\in M(X\times Y)} [-\int ud\pi - \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) - \varepsilon \int T(\pi)d\pi]$

成为一个光滑函数而不是一个指示函数.

在这里有一个技巧，那就是假设 $d\pi = R_\pi d\mu d\nu$, 也就是说 $d\pi$ admits a continuous density with respect to $ d\mu d\nu $. 也就是说，相当于我们加了一个新的约束： $\pi \in \{\pi: d\pi = R_\pi(x, y)d\mu d\nu \}$. 当然，我们并不是实际加了这个约束，只是在处理和 $\pi$ 相关的优化问题时尽量在这个集合内处理，以获得相应的启发. 如果实际加上这个约束的话，会导致 primal fenchel problem 的约束条件发生改变，我们就要重新处理整个框架了，不过也不是不能尝试（例如，考虑将这个约束加到 $g^*(-\pi)$ 里面，这样看起来好像 $f(u)$ 和 $g(u)$ 都没变？可以开一个专题讨论一下这个问题）.

这样上面的优化问题就可以转换为 $-\int \inf\limits_{R_\pi} [ uR_\pi + c(x, y)R_\pi + \varepsilon T(R_\pi)R_\pi] d\mu d\nu$, 因此，优化目标就变成了 $\inf\limits_{R_\pi} [ uR_\pi + c(x, y)R_\pi + \varepsilon T(R_\pi)R_\pi]$.

这时候要处理这个优化问题就非常简单了，因为我们成功把麻烦的测度变成了容易处理的函数了. 例如，我们可以看到 $T(R_\pi) = \log R_\pi$ 就能够避免 $\inf\limits_{R_\pi} [ uR_\pi + c(x, y)R_\pi + \varepsilon T(R_\pi)R_\pi]$

变成指示函数.
然后关于 $T(R_\pi)$ 的选取. 一个 general 的问题是，什么样的函数，与 sup 复合之后仍然是光滑的：

即对于多元光滑函数 $g(x_1,...,x_n)$, 我通过 sup 函数 reduce 其中一个变量，例如 $\hat{g}(x_1,..., \hat{x_i}, ...,x_n) = \sup\limits_{x_i}g(x_1,...,x_n)$

那么问题就是，$g(x_1,...,x_n)$ 除了光滑以外，还需要满足什么条件，使得 $\hat{g}(x_1,..., \hat{x_i}, ...,x_n)$ 也是光滑的？

仅凭光滑肯定是不够的，例如 $g(u, x) = (u+c)x$ 对于 $u$ 和 $x$ 来说，都是光滑的. 但是 $\sup\limits_x g(u, x)$ 和 $\sup\limits_u g(u, x)$ 均不是光滑的. 因此，即便是光滑的二元函数也无法保证 sup 归约变换也是光滑的. sup-光滑到底需要怎样的条件？肯定是一个很困难和苛刻的条件！

有一个结论，那就是 strongly convex 函数的凸共轭一定是光滑的. 所以 strongly convex > sup-光滑.

所以，我们现在的问题就是考虑一下存在哪些可选的 $T(R_\pi)$ s.t $ \sup\limits_{R_\pi}[- (u+c+T(R_\pi))R_\pi ] $ 或者 $\inf\limits_{R_\pi}[ (u+c+T(R_\pi))R_\pi ]$ 对于 $u$ 是光滑的.

~~选取 $T(R_\pi)$ 的时候要注意，$R_\pi \ge 0$.~~ 其实没必要要求 $ R_\pi \ge 0 $, 因为 $T(R_\pi)$ 又不一定包含类似 $\log R_\pi$ 的项. 上面的 inf 也没有要求 $ R_\pi \ge 0 $. 而且我们一开始在选定 $\pi$ 的取值空间的时候是 $\mathcal{M}(X, Y)$, 而不是 $\mathcal{M}_+(X, Y)$.

只不过是 EOT 中出现了 KL 散度，为了使得 KL 散度有意义，我们一般都在 $\mathcal{M}_+(X, Y)$ 上讨论，但是如果我们选择其他正则项，那就未必要求 $ \pi \in \mathcal{M}_+(X, Y) $ 了.

而且有一点，我必须要强调一下，省的混淆，那就是除了 $u=u^*$ 这一特殊情况外，求解
$\inf\limits_{R_\pi} [ uR_\pi + c(x, y)R_\pi + \varepsilon T(R_\pi)R_\pi]$

得到的最优的 $R_\pi$ 和 EOT primal formulation 中的最优的 $\pi$ 相互之间是没有联系的，务必不要混为一谈！在 inf 问题

$\inf\limits_{R_\pi} [ uR_\pi + c(x, y)R_\pi + \varepsilon T(R_\pi)R_\pi]$
中，$R_\pi$ 是可以取任何值的！！！

再强调一遍，在 dual fenchel problem 的优化目标转约束的过程中得到的最优的 $\pi$ 和 dual fenchel problem 的最优的 $\pi$ 是没有半毛钱关系的. 切记切记.

从论文中，我们已经得到了一个例子： $T(R_\pi) = \log R_\pi$, 除此之外呢？

我们先分析一下. 显然，要使 $\inf\limits_{R_\pi}[ (u+c+T(R_\pi))R_\pi ]$ 对于 $u$ 是光滑的，必须有 $ (u+c+T(R_\pi))R_\pi $ 最小值和 $u$ 相关或者是某个不为 $-\infty$ 的常量.

记 $S(u, R_\pi) = (u+c+T(R_\pi))R_\pi$, 则
$\frac{\partial S}{\partial R_\pi} = u+c+T(R_\pi) + R_\pi T'(R_\pi)$
令之为0，则有
$u+c+T(R_\pi) + R_\pi T'(R_\pi) = 0$
解一下这个方程，

$ (T(R_\pi)R_\pi)' = -(u+c) $ 求解这个方程的时候要注意，要把 $R_\pi$ 当作是一个普通的变量，而不是看作是函数. 你就是把 $R_\pi$ 看作是一个普通的变量，所有的求导和积分都是以 $R_\pi$ 为基本单位的. 如果看作是函数的话，那么上面的方程就变成了微分方程，当作微分方程来解也可以，但是这时候复杂度就有点高了，而且问题的形式也有点奇怪.

但是此时我们有 $S(u, R_\pi) = (u+c+T(R_\pi))R_\pi = - R_\pi T'(R_\pi) R_\pi$
由于我们希望 $S(u, R_\pi)$ 存在极小值，并且最好是唯一的极小值，所以对 $T(R_\pi)$ 会有一些要求：

1) $[T(R_\pi)R_\pi]' = T(R_\pi) + R_\pi T'(R_\pi)$ 应该是单增的， with respect to $R_\pi$.

这样一来，轻松就能保证唯一极小值（导数左边负右边正）
2) $[T(R_\pi)R_\pi]'$ 应该是全空间取值的. $R_\pi \in C_b(X, Y)\cap \text{dom} T$.

这样一来，无论 $u$ 取什么值，都存在 $R_\pi$ s.t $u+c+T(R_\pi) + R_\pi T'(R_\pi) = 0$. 这时候，$\inf\limits_{R_\pi} S(u, R_\pi)$ 就不需要对 $u$ 进行分段讨论了，因此 $u$ 是无约束的. 这样一来，$g(u)$ 就不再是分段函数（甚至指示函数），很可能是光滑的，thus primal fenchel problem 中关于 $u$ 的约束就不再存在了. 当然我们并不是一定拒绝分段函数，如果分段函数的形式简单，有利于优化算法，那么也是可以选择的，但是相对来说，我们 prefer 光滑函数.

从论文中，我们已经得到了一个例子： $T(R_\pi) = \log R_\pi$, 除此之外呢？例如令 $T(R_\pi) = (R_\pi)^n$? 我们可以分析一下.

1) 当 $n > 0$ 且 $n = 2k + 1$ 时. 以 $n = 1$ 为例. 对于 $S(u, R_\pi) = (u+c+\varepsilon R_\pi)R_\pi$, 那么有最小值 $ -\frac{(u+c)^2}{4\varepsilon} $, 此时 $ R_\pi = -\frac{u+c}{2\varepsilon} $.

对应地，我们有 $ R_\pi^* = -\frac{u^*+c}{2\varepsilon} $. 请注意，由于加入了正则项，此时的 dual ROT 中的 $u=-(\varphi+\psi)$ 是没有任何约束的，也就是说 $ u^* = -(\varphi^*+\psi^*) \le -c $ 是完全可能的。因此 $ -\frac{u^*+c}{2\varepsilon} $ 很可能是正的，$R_\pi^* = -\frac{u^*+c}{2\varepsilon}$ 很可能是一个可行的 optimal transport plan.

因此，这也是一个好的正则项的选择. 这个好像有一个名字叫做 $\chi^2$ divergence. 参考论文的这一段：

![](https://raw.githubusercontent.com/ai-math/picbed/master/1546837287186_4.png)

并且这时候我们看到了 $\iota_{U_c}$ 的另一个光滑逼近系列了：$ -\frac{(u+c)^2}{4\varepsilon} $

2) 当 $n > 0$ 且 $n = 2k$ 时. 以 $ n = 2 $ 为例. 对于 $S(u, R_\pi) = (u+c+R_\pi^2)R_\pi$ 来说，这种情况没有什么好讨论的了，$\inf S(u, R_\pi)$ 肯定是 $-\infty$. 对应地 $R_\pi$ 接近于取 $-\infty$.

3) 当 $n < 0$ 时. 令 $ k = -n$. $S(u, R_\pi) = (u+c+1/R_\pi^k)R_\pi = (u+c)R_\pi + \frac{1}{R_\pi^{k-1}}$.

这个也不用想了，$\inf S(u, R_\pi)$ 肯定是 $-\infty$. 对应地 $R_\pi$ 接近于取 $+\infty$ 或 $-\infty$.

### Semi-dual OT

下面证明一下 $\mathcal{D_\varepsilon} = \mathcal{S_\varepsilon}$. 对于 $S(u, R_\pi) = (u+c+R_\pi)R_\pi$

整理一下式子，可得：

$\mathcal{D_\varepsilon} = \max \int_\mathcal{Y} vd\nu + \int_\mathcal{X} [u - (\varepsilon\int_\mathcal{Y}\exp(\frac{v-c}{\varepsilon}) d\nu)\exp(\frac{u}{\varepsilon})] d\mu $

在 $\mathcal{D_\varepsilon}$ 中对 $u$ 求导，可得下面的式子

$ 1 - \int_\mathcal{Y}\exp(\frac{v-c}{\varepsilon}) d\nu \exp(\frac{u}{\varepsilon}) = 0$

可得 $ u = -\varepsilon \log \int_\mathcal{Y}\exp(\frac{v-c}{\varepsilon}) d\nu = v^{c, \varepsilon} $

将该式代回 $\mathcal{D_\varepsilon}$ 可得 $\mathcal{S_\varepsilon}$.

代回的过程还是很有意思的，此时 $\int_\mathcal{X\times Y}\exp(\frac{v^{c, \varepsilon} + v-c}{\varepsilon}) d\mu d\nu = \frac{\int_\mathcal{Y}\exp(\frac{v-c}{\varepsilon}) d\nu}{\int_\mathcal{Y}\exp(\frac{v-c}{\varepsilon}) d\nu} = 1$

而我们可以看到，如果有 $v^{c, \varepsilon}(x) + v(y) = c(x, y)$, 也能使得上式为 1. 但是一般来说，$v^{c, \varepsilon}(x) + v(y) \ne c(x, y)$, 但是仍然保持了上式为 1.

这一点非常有趣，内中到底有什么奥妙，使得这一整套系统的构建如此的有规律？

### 洞察光滑逼近

看论文的时候，我觉得怎么这些光滑逼近选的这么好，恰好和 EOT 的 dual problem 联系起来. 但是我突然反应过来，反了，其实是 EOT 决定了这些光滑逼近。下面我解释一下这些点.

我们考虑一个一般的 regularized OT, 简称 ROT,

那么 regularized primal OT 的形式就是 $ \inf\limits_{\pi\in \mathcal{\Pi(X\times Y)}} \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon R(\pi) $

同样地通过一番拉格朗日乘子法，我们可以得到其 regularized dual OT. 由对称性，在 regularized dual OT 中肯定是有一项用于 regularize $u$ 和 $v$ 的，不妨记为 $ T^\varepsilon(u, v) $. $ T^\varepsilon(u, v) $ 肯定是凸的，并且肯定会有 $ T^\varepsilon(u, v) $ converge to $ \iota_{U_c} $ with $\varepsilon \to 0$.

其实这一点很容易看出来：

$ T^\varepsilon(-u) = \sup [ \langle -u, \pi angle - \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon R(\pi) ] $

而我们知道，当 $\varepsilon\to 0$ 时，有 $ T^\varepsilon(-u) = \sup [ \langle -u, \pi angle - \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y) + \varepsilon R(\pi) ] \\\to \sup [ \langle -u, \pi angle - \int_{\mathcal{X\times Y}} c(x, y)d\pi(x, y)] = \iota_{U_c} $.

其实还有更简单的看出这一点的方法，从正则项的选取那一节，我们知道，其实 dual ROT 的约束就是正则项 $\varepsilon R(\pi)$ 的凸共轭的一个复合：

$ T^\varepsilon(-u) = (\varepsilon R)^*(-u-c) $

而当 $\varepsilon\to 0$ 时，$\varepsilon R(\pi) \to 0$, 从而 $(\varepsilon R))^* \to 0^*$, 从而 $ T^\varepsilon(-u) \to 0^*(-u-c) $.

当我选 $R(\pi)$ 为 $\chi^2$ divergence 的时候，有 $\iota_{U_c}$ 的另一个光滑逼近系列：$ -\frac{(u+c)^2}{4\varepsilon} $.

## EOT 和 OT 的 KKT 条件的对比

对于 OT 来说，KKT条件并不能提供太多的信息. 对于离散的case，单单是等式 $\inf \langle P, Mangle = \sup u^T\mu + v^T\nu$ 就能够给出 $P$ 和 $u, v$ 之间的关系，好像是在哪看到的，忘记了. 可能是 computational OT 那本书.

对于 EOT 来说，我们从 KKT 条件看到了，$\pi$ 和 $u, v$ 直接存在显式的关系，这就非常方便了，通过计算对偶问题，不仅可以得到 OT distance，还可以直接得到 optmial transport plan, 而不需要其他的额外的计算.

但是在 original OT 中，一般来说，算完对偶问题得到 OT distance 后，我们一般还需要利用这个 OT distance 经过额外的估计计算以后才能获得 OT plan.

## 关键词

convex conjugate AISTATS的那篇论文用的很多. 记得总结一下.
KKT condition
lagrange dual
Perturbation Function
写作计划：fenchel duality(convex duality), Perturbation Function

## 参考资料

https://en.wikipedia.org/wiki/Duality_(optimization)

https://en.wikipedia.org/wiki/Wolfe_duality

https://en.wikipedia.org/wiki/Fenchel%27s_duality_theorem

https://en.wikipedia.org/wiki/Strong_duality

https://en.wikipedia.org/wiki/Perturbation_function

https://en.wikipedia.org/wiki/Linear_programming

http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/12-kkt-scribed.pdf Lecture 12: KKT Conditions

https://optimaltransport.github.io/pdf/ComputationalOT.pdf 和我一样，使用了拉格朗日乘子法来处理 duality 问题，但是这本书讨论的是离散OT的case. 事实上，这本书的绝大部分篇幅都是在将离散的OT，当然连续的case可以简单地从离散的case推广得到.</article>
</body>
<script src="https://static.outiy5.com/js/adv_dev.js?css=table1,basic;js=mathjax;mod=md"></script>
</html>